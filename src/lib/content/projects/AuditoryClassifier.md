# Auditory Classifier

An intelligent audio classification system using deep learning for real-time sound recognition.

## üîó Links

- **GitHub**: [github.com/eshan327/auditory-classifier](https://github.com/eshan327/auditory-classifier)
- **Research Paper**: [View on arXiv](https://arxiv.org/abs/example)
- **Demo Video**: [YouTube](https://youtube.com/watch?v=example)

## üìã Overview

Developed a convolutional neural network-based system that classifies audio signals into 50+ distinct categories in real-time. The model achieves 94.2% accuracy on the UrbanSound8K dataset and can process audio streams with &lt;100ms latency.

## üõ†Ô∏è Tech Stack

**Machine Learning:**
- Python 3.10
- PyTorch for model architecture
- TensorFlow/Keras for prototyping
- scikit-learn for preprocessing

**Audio Processing:**
- librosa for feature extraction
- soundfile for audio I/O
- pydub for format conversion

**Deployment:**
- FastAPI for REST API
- Docker containerization
- AWS EC2 for inference server
- Redis for caching predictions

**Data & Visualization:**
- NumPy and Pandas for data manipulation
- Matplotlib and Seaborn for analysis
- Weights & Biases for experiment tracking

## ‚ú® Key Features

- **Real-time Classification**: Process and classify audio streams with 70-100ms latency
- **Multi-label Detection**: Identify multiple simultaneous sounds in complex audio scenes
- **Noise Robustness**: Maintains 87% accuracy even with 20dB SNR background noise
- **Transfer Learning**: Fine-tuned on custom datasets with as few as 100 samples per class
- **Confidence Scoring**: Provides calibrated probability scores for each prediction
- **Explainable AI**: Visualizes which audio frequencies influenced classification decisions

## üìä Impact & Results

- **94.2% accuracy** on UrbanSound8K benchmark (top 5% of published results)
- **91.3% accuracy** on ESC-50 environmental sounds dataset
- **87.8% accuracy** on custom real-world dataset with noisy conditions
- Model size: **4.2MB** (lightweight enough for edge deployment)
- Inference time: **73ms** average on CPU, **12ms** on GPU
- Published findings at [University Research Symposium](https://umd.edu/research)

## üí° Technical Highlights

**Novel Architecture:**
- Designed custom CNN architecture combining mel-spectrogram and MFCC features
- Implemented attention mechanisms to focus on relevant time segments
- Used depthwise separable convolutions to reduce model parameters by 60%

**Data Augmentation:**
- Time stretching and pitch shifting for robust temporal invariance
- SpecAugment for frequency masking
- Added synthetic noise from MUSAN corpus
- Mixup technique for better generalization

**Training Optimizations:**
- Implemented cosine annealing learning rate schedule
- Used label smoothing to prevent overconfidence
- Applied gradient accumulation for effective batch size of 256
- Achieved convergence in ~15 epochs using AdamW optimizer

## üöÄ Challenges & Solutions

**Challenge**: Handling variable-length audio inputs efficiently
**Solution**: Implemented dynamic padding with attention masks, allowing batched processing without unnecessary computation on padded regions

**Challenge**: Class imbalance in real-world dataset (some sounds 10x rarer than others)
**Solution**: Combined focal loss with class-weighted sampling, improving rare class F1-score by 23%

**Challenge**: Deployment latency too high for real-time use
**Solution**: 
- Quantized model to INT8 using PyTorch's dynamic quantization (3.2x speedup)
- Implemented sliding window with overlap-add for streaming audio
- Added Redis caching for repeated similar inputs

## üìà Use Cases

- **Smart Home**: Detect glass breaking, smoke alarms, baby crying
- **Industrial Monitoring**: Identify machine malfunctions by abnormal sounds
- **Wildlife Research**: Classify bird calls and animal vocalizations
- **Accessibility**: Audio event detection for hearing-impaired users

## üî¨ Research Contributions

- Explored effectiveness of different audio representations (raw waveform vs. spectrograms)
- Compared CNN, RNN, and Transformer architectures for audio tasks
- Analyzed impact of sample rate on classification accuracy
- Investigated cross-dataset generalization challenges

## üìö Future Work

- Implement few-shot learning for rapid adaptation to new sound categories
- Develop mobile app for on-device classification (iOS/Android)
- Create annotated dataset for rare emergency sounds
- Explore self-supervised pre-training on YouTube-8M audio subset
